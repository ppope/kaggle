#This script is an exercise in using Ensemble modeling based on a tutorial which can be found at 
#http://www.overkillanalytics.net/more-is-always-better-the-power-of-simple-ensembles/


#################################################################
#################################################################
#################################################################
# Chunk 1
#   Here the signal function is defined as g(x,y)= 1 / (1 + 2^(x^3+ y+x*y) on (-2,2).
#   The training data is generated by randomly selecting 10000 samples from a N(0,1) distribution. 
#   The class membership of each samples is decided by comparing a random draw from the uniform distribution with g(x1,x2)
#   The dataset also includes two irrelevant random features (x3 and x4) to be used as noise. 
#   The plot of the training set is constructed on a 25x25 grid on (-2,2) by cutting up the data into intervals and taking their mean.

# packages
require(fields) # for heatmap plot with legend
require(randomForest) # requires installation, for random forest models
set.seed(20120926)

# heatmap wrapper, plotting func(x, y) over range a by a
hmap.func <- function(a, f, xlab, ylab) 
{
  image.plot(a, a, outer(a, a, f), zlim = c(0, 1), xlab = xlab, ylab = ylab)
}

# define class signal
g <- function(x, y) 1 / (1 + 2^(x^3+ y+x*y))

# create training data
d <- data.frame(x1 = rnorm(10000), x2 = rnorm(10000)
                ,x3 = rnorm(10000), x4 = rnorm(10000))
d$y = with(d, ifelse(runif(10000) < g(x1, x2), 1, 0))

# plot signal (left hand plot below)
a = seq(-2, 2, len = 100)
hmap.func(a, g, "x1", "x2")

# plot training data representation (right hand plot below)
z = tapply(d$y,  list(cut(d$x1, breaks = seq(-2, 2, len=25))
                      , cut(d$x2, breaks = seq(-2, 2, len=25))), mean)
image.plot(seq(-2, 2, len=25), seq(-2, 2, len=25), z, zlim = c(0, 1)
           , xlab = "x1", ylab = "x2")


#################################################################
#################################################################
#################################################################
#   Chunk 2 fits logistic regression and random forest models to the data.

# Fit log regression and random forest
fit.lr = glm(y~x1+x2+x3+x4, family = binomial, data = d)
fit.rf = randomForest(as.factor(y)~x1+x2+x3+x4, data = d, ntree = 100, proximity = FALSE)

# Create functions in x1, x2 to give model predictions
# while setting x3, x4 at origin
g.lr.sig = function(x, y) predict(fit.lr, data.frame(x1 = x, x2 = y, x3 = 0, x4 = 0), type = "response") 
g.rf.sig = function(x, y) predict(fit.rf, data.frame(x1 = x, x2 = y, x3 = 0, x4 = 0), type = "prob")[, 2] 
g.en.sig = function(x, y) 0.5*g.lr.sig(x, y) + 0.5*g.rf.sig(x, y)

# Map model predictions in x1 and x2
hmap.func(a, g.lr.sig, "x1", "x2")
hmap.func(a, g.rf.sig, "x1", "x2")
hmap.func(a, g.en.sig, "x1", "x2")

#################################################################
#################################################################
#################################################################
#   Chunk 3 tests the effects of noise on the logistic regression and random forest models separately.
#   Note that the random forest is much more sensitive to noise than logistic regression.


# Create funtions in x3, x4 to give model predictions
# while setting x1, x2 at origin
g.lr.noise = function(x, y) predict(fit.lr, data.frame(x1 = 0, x2 = 0, x3 = x, x4 = y), type = "response")
g.rf.noise = function(x, y) predict(fit.rf, data.frame(x1 = 0, x2 = 0, x3 = x, x4 = y), type = "prob")[, 2] 
g.en.noise = function(x, y) 0.5*g.lr.noise(x, y) + 0.5*g.rf.noise(x, y)

# Map model predictions in noise inputs x3 and x4
hmap.func(a, g.lr.noise, "x3", "x4")
hmap.func(a, g.rf.noise, "x3", "x4")
hmap.func(a, g.en.noise, "x3", "x4")


#################################################################
#################################################################
#################################################################
#   Chunk 4 creates a plot of cross entropy error against how much weight in the ensemble is given to logistic regression.


# (Ugly) function for measuring cross-entropy error
cross.entropy <- function(target, predicted)
{
  predicted = pmax(1e-10, pmin(1-1e-10, predicted))
  - sum(target * log(predicted) + (1 - target) * log(1 - predicted))
}

# Creation of validation data
dv <- data.frame(x1 = rnorm(10000), x2 = rnorm(10000)
                 , x3 = rnorm(10000), x4 = rnorm(10000))
dv$y = with(dv, ifelse(runif(10000) < g(x1, x2), 1, 0))

# Create predicted results for each model
dv$y.lr <- predict(fit.lr, dv, type = "response")
dv$y.rf <- predict(fit.rf, dv, type = "prob")[, 2]

# Function to show ensemble cross entropy error at weight W for log. reg.
error.by.weight <- function(w) cross.entropy(dv$y, w*dv$y.lr + (1-w)* dv$y.rf)

# Plot + pretty
plot(Vectorize(error.by.weight), from = 0.0, to = 1,
     xlab = "ensemble weight on logistic regression", ylab = "cross-entropy error of ensemble", col = "blue")
text(0.1, error.by.weight(0)-30, "Random\nForest\nOnly")
text(0.9, error.by.weight(1)+30, "Logistic\nRegression\nOnly")